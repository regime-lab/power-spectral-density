{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size=\"6\">Fractional Processes & Power Spectral Density</font>\n\nThe goal of this notebook is to make a maximum likelihood estimator that takes into account the auto-correlation of the data, not just mean and variance or other statistical moments. The starting point for that would seem to be some kind of distance metric, but this is fundamentally what I need help with or to know if it's even possible. \n\nEven if two sets of data have the same distributional parameters the order of the data in terms of variance or some other auto-correlated parameter may not be the same. This is the auto-correlation decay in the time domain. \n\nProcesses with long memory or periodic memory will not be ergodic especially if the auto-correlation decay is slow and follows a power law. So is it possible to have a maximum likelihood estimator that would compute likelihoods for the mean, variance, and auto-covariance of the data considered all together?\n\n<font size=\"5\">RBF Kernel</font>\n\nInspired partially by the work of these researchers,\nhttp://cermics.enpc.fr/~guyon/documents/VolatilityIsMostlyPathDependent_Slides_RiO2022_22Aug2022.pdf\n\nIn this paper they discuss how kernels can encode the long or short-term memory of the volatility dynamics. I have some other papers that discuss similar ideas. \n\n<font size=\"5\">Use Case in Factor Analysis</font>\n\nCan Factor Investing Become Scientific? Endogenous Volatility. \n\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4205613\n\nIrrational Capital is a good example of a research/fund looking at deep ESG metrics, \nhttps://www.irrational.capital/","metadata":{},"id":"5bd758d2-5355-4a2e-a083-b9fc6eb16921"},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport sklearn \nimport sklearn.mixture as mix ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ad0be99d-726d-404c-8361-341b6bf14964"},{"cell_type":"code","source":"assetlist = [ 'IEF', 'GSG', 'IXN' ]\nnum_components = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"19611689-4dab-4998-9d4b-85a3e367925b"},{"cell_type":"markdown","source":"Create the subset of assets. We end up with a 3-dimensional multivariate gaussian system. ","metadata":{},"id":"e574a932-24a5-4ed9-be31-944af7497a96"},{"cell_type":"code","source":"m6 = pd.read_csv('./data/assets.csv')\nm6_assets = pd.DataFrame()\n\nfor sym in assetlist: \n    m6_assets[sym] = m6[m6['symbol'] == sym]['price'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2eb94979-6731-47a0-b644-e08b1bee1e6c"},{"cell_type":"markdown","source":"<font size=\"5\">Z-Scoring & Auto-Correlation Decay Plots</font>\n\nFormatting the training data. There is only training data and no test data in this exercise (it is a purely descriptive exercise). Z-scored measurements of the log price for each asset based on a rolling window. At timepoint 't' the window looks back 'w' steps to calculate mu and sigma. \n\n$$\\huge z_t = \\frac{x_t - \\mu_{t-w:t}}{\\sigma_{t-w:t}}$$\n\nThere are regions where the current log price is either above or below its running average (as measured in units of standard deviation) and this formatting of the data preserves long memory based on looking at the auto-correlation decay plots. <b>It is important that the running average never takes into account future data which would introduce lookahead bias, so a rolling metric based on the list comprehension is used.</b> \n\n(Hurst Exponent Auto-Covariance Function) \n\n$$\\huge cov(s,t) = \\frac{1}{2} \\left( t^{2H} + s^{2H} - |t-s|^{2H} \\right)$$","metadata":{},"id":"11d474de-0f70-41be-9442-1cbe64ea5a62"},{"cell_type":"code","source":"import scipy.stats as stats\nimport scipy \n\nm6_subset1 = m6_assets.copy()\n\n# Window length 5 days\nwlen = 45\n\n# First convert to log price \nm6_subset1 = m6_subset1.apply(np.log)\n\n# Clean data\nm6_subset1 = m6_subset1.dropna().reset_index().drop(columns='index')\n\n# Apply z-score in a rolling way that does not create lookahead bias \nrolling_zscore = lambda serie: [ stats.zscore(serie[x-wlen:x]).values[-1] for x in range(wlen, len(serie) + 1) ]\n\nm6_subset = pd.DataFrame()\nm6_subset['IEF'] = m6_subset1['IEF'].diff().dropna()\nm6_subset['GSG'] = m6_subset1['GSG'].diff().dropna()\nm6_subset['IXN'] = m6_subset1['IXN'].diff().dropna()\n\n#m6_subset['IEF'] = rolling_zscore(m6_subset1['IEF'])\n#m6_subset['GSG'] = rolling_zscore(m6_subset1['GSG'])\n#m6_subset['IXN'] = rolling_zscore(m6_subset1['IXN'])\n\n# View Data\nsns.lineplot(data=m6_subset)\nplt.show() \n\n# View Auto Correlation\ndecay_func = lambda S, _range: [S.autocorr(lag=Lag) for Lag in _range]\nsns.lineplot(data=decay_func(m6_subset['IEF'], range(0, 100)), label='autocorr_IEF', marker='o') \nsns.lineplot(data=decay_func(m6_subset['GSG'], range(0, 100)), label='autocorr_GSG', marker='o')\nsns.lineplot(data=decay_func(m6_subset['IXN'], range(0, 100)), label='autocorr_IXN', marker='o')\nplt.xlabel('Lag')\nplt.ylabel('auto-correlation')\nplt.show()\n\n%pip install torch\n%pip install gpytorch\n%pip install git+https://github.com/forrestbao/pyeeg.git\nimport torch\nimport gpytorch\nimport pyeeg\n\nclass FractionalPSD:\n    def __init__(self, data):\n        self.data = np.array(data)\n\n    def distance_norm(self, target): \n        x1 = torch.tensor([ _x for _x in target ], dtype=torch.float)\n        x0 = torch.tensor([ _x for _x in self.data  ], dtype=torch.float)\n        \n        # Evaluate kernel auto-covariance matrix \n        kernel = gpytorch.kernels.RBFKernel()\n        autocov_1 = (kernel(x1).evaluate()).detach().numpy() \n        autocov_0 = (kernel(x0).evaluate()).detach().numpy()\n\n        # Frobius distance between kernel evaluations \n        return np.linalg.norm(autocov_1 - autocov_0, 'fro') \n    \n# Example comparing white noise with same mean/variance to fractional processes \nfractional_ief = FractionalPSD(m6_subset['IEF'].values)\nfractional_gsg = FractionalPSD(m6_subset['GSG'].values)\nfractional_ixn = FractionalPSD(m6_subset['IXN'].values)\nscores_ief=[]\nscores_gsg=[]\nscores_ixn=[]\nscores=[]\nhursts=[]\n\nwhite_noise = torch.normal(np.mean(fractional_ief.data), np.var(fractional_ief.data), size=(len(m6_subset),))\nwhite_noise = white_noise.detach().numpy()\nhursts.append(pyeeg.hurst(white_noise))\nwhite_noise = FractionalPSD(white_noise)\n\nscores_ief.append(white_noise.distance_norm(fractional_ief.data))\nscores_gsg.append(white_noise.distance_norm(fractional_gsg.data))\nscores_ixn.append(white_noise.distance_norm(fractional_ixn.data))\nprint(scores_ief)\nprint(scores_gsg)\n\nplt.scatter(x=range(len(scores_ief)), y=scores_ief, s=8, label='scores_ief')\nplt.scatter(x=range(len(scores_gsg)), y=scores_gsg, s=8, label='scores_gsg')\nplt.scatter(x=range(len(scores_ixn)), y=scores_ixn, s=8, label='scores_ixn')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ef1b8cf6-43b1-4593-b52d-bc949ed27e7e"},{"cell_type":"markdown","source":"Instantiate a non-parametric model using the dirichlet process and view the clustering results on the training data. The shaded regions on the plot represent the final state the system lands on, states[-1]. ","metadata":{},"id":"df7da765-fdf4-4c33-99a8-ecf5c1e5ef5c"},{"cell_type":"code","source":"dpgmm_model = mix.BayesianGaussianMixture(n_components=num_components, \n                                          weight_concentration_prior_type='dirichlet_process', \n                                          n_init=1, \n                                          max_iter=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"93d452d7-f21e-4383-b49b-1b4b22cfdba8"},{"cell_type":"code","source":"dpgmm_model.fit(m6_subset)\nstates = dpgmm_model.predict(m6_subset)\n\n# Count the frequency of each latent state \nstate_counts = np.zeros(num_components)\nfor M in states:\n  state_counts[M] += 1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"698175ae-6139-46e0-bfd6-eb68384a8b2b"},{"cell_type":"markdown","source":"<font size=5> Precision Matrices as Regime-Specific Graphs </font>\n\nThe precision matrix is just the inverse of the covariance matrix. Connections between nodes representing degree of dependence/independence within each respective latent state regime. Nodes are one dimension (feature) of the data. ","metadata":{},"id":"32e79c7c-937f-49c9-80d9-c6edd305ccfe"},{"cell_type":"code","source":"%pip install networkx \nimport networkx as nx\nimport matplotlib.pyplot as plt \n\nfig, axes = plt.subplots(2, 2, figsize=(8, 5))\n\ni = 0\nj = 0 \nfor pmat in dpgmm_model.precisions_:\n    if state_counts[i] > 0: \n        ax = axes[j//2][j%2]\n        G = nx.DiGraph(pmat)\n        nx.draw(G, with_labels=True, ax=ax)\n        j += 1\n    i += 1\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5cc81611-ce59-4b28-920e-9e9c1f846048"}]}
